{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de0f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_PATH = \"emails.csv\"  # Update with your dataset path\n",
    "OUTPUT_DIR = \"output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7813dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in .env file\")\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb90cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini model\n",
    "gemini_model = genai.GenerativeModel('models/gemini-1.5-flash-latest')\n",
    "gemini_evaluation_model = genai.GenerativeModel('models/gemini-2.5-flash-preview-04-17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"Load the Enron email dataset\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded with {len(df)} emails\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e4949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_email(text):\n",
    "    \"\"\"Clean and preprocess email text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove all known headers\n",
    "    text = re.sub(r\"(?i)^((Message-ID|Date|From|To|Subject|Cc|Bcc|Mime-Version|Content-Type|Content-Transfer-Encoding|X-[\\w-]+):\\s.*\\n)+\", '', text)\n",
    "    \n",
    "    # Remove signature starting with common delimiter\n",
    "    text = re.sub(r\"(?i)(\\n)?(--|__|Regards,|Best,|Thanks,|Sincerely).*\", '', text)\n",
    "\n",
    "    # Remove quoted replies\n",
    "    text = re.sub(r\"(?s)(>+\\s?.*\\n)+\", '', text)\n",
    "\n",
    "    # Normalize spacing\n",
    "    text = re.sub(r\"\\s+\", ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_email_thread(emails):\n",
    "    \"\"\"Summarize a thread of emails using Gemini\"\"\"\n",
    "    combined_text = \"\\n\\n\".join([preprocess_email(email) for email in emails])\n",
    "    \n",
    "    if len(combined_text.split()) < 50:\n",
    "        return \"Thread too short for meaningful summary.\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a professional assistant tasked with summarizing a multi-message email thread.\n",
    "\n",
    "    Your goal is to provide a clear, actionable summary suitable for busy professionals. Focus strictly on the following:\n",
    "\n",
    "    - **Key decisions made** (if any)\n",
    "    - **Action items and owners** (who needs to do what)\n",
    "    - **Core discussion points** (main topics or concerns raised)\n",
    "\n",
    "    Guidelines:\n",
    "    - Write in bullet points or a short structured paragraph.\n",
    "    - Exclude greetings, signatures, and irrelevant small talk.\n",
    "    - Do not include any meta-comments about summarizing.\n",
    "    - Only return the summary content â€” no explanations or preambles.\n",
    "    \n",
    "    Email Thread:\n",
    "    {combined_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summary: {e}\")\n",
    "        return \"Summary generation failed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8eca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_email_response(email_text):\n",
    "    \"\"\"Generate a response to an email using Gemini\"\"\"\n",
    "    cleaned_text = preprocess_email(email_text)\n",
    "    email_type = identify_common_email_type(cleaned_text)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an executive assistant generating email replies in a professional business context.\n",
    "\n",
    "    Objective:\n",
    "    Write a polite, concise, and contextually appropriate 3-5 sentence response to the following email.\n",
    "\n",
    "    Email type: **{email_type}**\n",
    "    Tone: Professional and courteous  \n",
    "    Length: 3-5 sentences  \n",
    "    Constraints:\n",
    "    - Do not repeat the sender's message.\n",
    "    - Do not add generic disclaimers or unrelated content.\n",
    "    - Focus on addressing the email's core intent clearly.\n",
    "\n",
    "    Email Content:\n",
    "    \\\"\\\"\\\"\n",
    "    {cleaned_text[:5000]}\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    Your Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"Response generation failed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128dcb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_common_email_type(email_text):\n",
    "    \"\"\"Simple classifier to identify common email types\"\"\"\n",
    "    email_text = email_text.lower()\n",
    "    \n",
    "        # Define keyword categories\n",
    "    keyword_map = {\n",
    "        \"meeting request\": ['meeting', 'schedule', 'appointment', 'calendar', 'reschedule', 'invite'],\n",
    "        \"status update\": ['status', 'update', 'progress', 'report', 'milestone', 'summary'],\n",
    "        \"information request\": ['question', 'help', 'advice', 'suggestion', 'clarify', 'inquire'],\n",
    "        \"thank you note\": ['thank', 'thanks', 'appreciate', 'grateful', 'gratitude'],\n",
    "        \"follow-up\": ['follow up', 'just checking', 'circling back', 'ping', 'reminder'],\n",
    "        \"issue report\": ['bug', 'issue', 'error', 'problem', 'fail', 'failure', 'glitch'],\n",
    "        \"approval request\": ['approve', 'approval', 'authorize', 'permission', 'sign off']\n",
    "    }\n",
    "\n",
    "    # Check for category matches\n",
    "    for category, keywords in keyword_map.items():\n",
    "        if any(kw in email_text for kw in keywords):\n",
    "            return category\n",
    "\n",
    "    return \"general inquiry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a58b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summary_quality(original_emails: List[str], summary: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the quality of a summary against the original emails.\n",
    "    Returns a dictionary with evaluation metrics.\n",
    "    \"\"\"\n",
    "    combined_original = \"\\n\\n\".join([preprocess_email(email) for email in original_emails])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert communication analyst. Your task is to evaluate a **generated email summary** based on the content of the **original email threads**.\n",
    "\n",
    "    Use the following criteria, scoring each from **0.0 to 1.0** (where 1.0 = perfect, 0.0 = poor):\n",
    "\n",
    "    [SCORING CRITERIA]\n",
    "    1. **main_points_coverage** - Does the summary include all key ideas and important content from the emails?\n",
    "    2. **conciseness** - Is the summary brief while still preserving essential meaning?\n",
    "    3. **accuracy** - Are the facts and interpretations in the summary correct according to the original emails?\n",
    "    4. **action_clarity** - Are any action items or next steps clearly and understandably summarized?\n",
    "    5. **structure** - Is the summary logically organized and easy to follow?\n",
    "\n",
    "    [INPUT DATA]\n",
    "    Original Emails:\n",
    "    {combined_original[:10000]}\n",
    "\n",
    "    Generated Summary:\n",
    "    {summary}\n",
    "\n",
    "    Evaluation (return only a Python dictionary with float values from 0 to 1 and no other text):\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = gemini_evaluation_model.generate_content(prompt)\n",
    "        print(\"\\n--- Summary Evaluation:---\\n\", response.text)  # Debug\n",
    "        evaluation = response.text.strip()\n",
    "        return evaluation\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating summary: {e}\")\n",
    "    \n",
    "    # Return default scores if evaluation fails\n",
    "    return {\n",
    "        'main_points_coverage': 0.1,\n",
    "        'conciseness': 0.1,\n",
    "        'accuracy': 0.1,\n",
    "        'action_clarity': 0.1,\n",
    "        'structure': 0.1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_quality(original_email: str, response: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the quality of a generated response.\n",
    "    Returns a dictionary with evaluation metrics.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert email communication evaluator.\n",
    "\n",
    "    Your task is to evaluate the **generated email response** using the following five criteria. Each should be scored individually on a float scale from 0.0 to 1.0 (inclusive), where:\n",
    "    - 1.0 = perfect\n",
    "    - 0.0 = poor\n",
    "\n",
    "    [RETURN FORMAT REQUIREMENT]\n",
    "    Return ONLY a **valid Python dictionary**, using this exact structure:\n",
    "    {{\n",
    "        'coherence': float,\n",
    "        'contextual_appropriateness': float,\n",
    "        'relevance': float,\n",
    "        'professionalism': float,\n",
    "        'actionability': float\n",
    "    }}\n",
    "\n",
    "    [CRITERIA DEFINITION]\n",
    "    1. **coherence**: Is the response logically structured and easy to follow?\n",
    "    2. **contextual_appropriateness**: Does the tone and content fit the original email's context?\n",
    "    3. **relevance**: Does it appropriately address the content of the original email?\n",
    "    4. **professionalism**: Is the tone courteous, formal where appropriate, and free of slang?\n",
    "    5. **actionability**: Are clear next steps or calls to action included if necessary?\n",
    "\n",
    "    [INPUT]\n",
    "    Original Email:\n",
    "    \\\"\\\"\\\"{original_email[:5000]}\\\"\\\"\\\"\n",
    "\n",
    "    Generated Response:\n",
    "    \\\"\\\"\\\"{response}\\\"\\\"\\\"\n",
    "\n",
    "    [IMPORTANT]\n",
    "    - DO NOT explain your ratings.\n",
    "    - DO NOT return anything except the dictionary. \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = gemini_evaluation_model.generate_content(prompt)\n",
    "        print(\"\\n--- Response Evaluation:--- \\n\", response.text)\n",
    "        # Safely evaluate the dictionary response\n",
    "        evaluation = response.text.strip()\n",
    "        return evaluation\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating response: {e}\")\n",
    "    \n",
    "    # Return default scores if evaluation fails\n",
    "    return {\n",
    "        'coherence': 0.1,\n",
    "        'contextual_appropriateness': 0.1,\n",
    "        'relevance': 0.1,\n",
    "        'professionalism': 0.1,\n",
    "        'actionability': 0.1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35288801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_evaluation_results(evaluation: Dict[str, float], prefix: str = \"\") -> str:\n",
    "    \"\"\"Format evaluation metrics for display\"\"\"\n",
    "    formatted = []\n",
    "    for key, value in evaluation.items():\n",
    "        formatted_key = prefix + key.replace('_', ' ').title()\n",
    "        formatted.append(f\"{formatted_key}: {value:.2f}/1.00\")\n",
    "    return \"\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84967ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load dataset\n",
    "    df = load_dataset(DATASET_PATH)\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # Example 1: Summarize a thread of 3 emails\n",
    "    # print(\"\\n=== Email Summarization Example ===\")\n",
    "    sample_thread = df['message'].sample(3).tolist()\n",
    "    # print(\"\\nOriginal Thread (first email snippet):\")\n",
    "    # print(sample_thread[0][:200] + \"...\")\n",
    "    \n",
    "    summary = summarize_email_thread(sample_thread)\n",
    "    print(\"\\n=== Generated Summary=== :\")\n",
    "    print(summary)\n",
    "\n",
    "    # Evaluate summary quality\n",
    "    summary_evaluation = evaluate_summary_quality(sample_thread, summary)\n",
    "    \n",
    "    \n",
    "    # Example 2: Generate response to an email\n",
    "    # print(\"\\n=== Response Generation Example ===\")\n",
    "    sample_email = df['message'].sample(1).iloc[0]\n",
    "    # print(\"\\nOriginal Email snippet:\")\n",
    "    # print(sample_email[:200] + \"...\")\n",
    "    \n",
    "    response = generate_email_response(sample_email)\n",
    "    print(\"\\n=== Generated Response:=== \")\n",
    "    print(response)\n",
    "\n",
    "    # Evaluate response quality\n",
    "    response_evaluation = evaluate_response_quality(sample_email, response)\n",
    "    \n",
    "    # Save results with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"results_{timestamp}.txt\")\n",
    "    \n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        # Write original thread content\n",
    "        f.write(\"=== ORIGINAL THREAD ===\\n\")\n",
    "        for i, email in enumerate(sample_thread):\n",
    "            f.write(f\"\\n--- Email {i+1} ---\\n\")\n",
    "            f.write(email + \"\\n\")\n",
    "        # Write summary\n",
    "        f.write(\"\\n\\n=== GENERATED SUMMARY ===\\n\")\n",
    "        f.write(summary + \"\\n\")\n",
    "        \n",
    "        # Write original email for response\n",
    "        f.write(\"\\n\\n=== ORIGINAL EMAIL FOR RESPONSE ===\\n\")\n",
    "        f.write(sample_email + \"\\n\")\n",
    "        \n",
    "        # Write generated response\n",
    "        f.write(\"\\n=== GENERATED RESPONSE ===\\n\")\n",
    "        f.write(response + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nProcessing complete. Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321ab7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
